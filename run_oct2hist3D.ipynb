{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Use this notebook to convert OCT 3D volume (stored in .tiff)  to virtual histology.\n",
        "\n",
        "To get started,\n",
        "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/OCT2Hist-ModelInference/blob/main/run_oct2hist3D.ipynb) and run.\n"
      ],
      "metadata": {
        "id": "NQOdtmQ3laV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "# @title Notebook Inputs { display-mode: \"form\" }\n",
        "# @markdown Input Image Path\n",
        "# Path to the OCT image\n",
        "oct_input_image_path = \"/content/drive/Shareddrives/Yolab - Current Projects/Aidan/2023 Alopecia Test/AlopeciaTest-Region1.tif\" # @param {type:\"string\"}\n",
        "\n",
        "# OCT image's pixel size\n",
        "microns_per_pixel_z = 1\n",
        "microns_per_pixel_x = 1\n",
        "\n",
        "# @markdown Cropping Parameters\n",
        "x0 = 130 # @param {type:\"slider\", min:0, max:1000, step:10}\n",
        "z0 = 300 # @param {type:\"slider\", min:0, max:1000, step:10}"
      ],
      "metadata": {
        "id": "uSag75ONOOgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Environment"
      ],
      "metadata": {
        "id": "UNHZ8Sbinu1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'setup_env_oct2Hist_usemodel' in globals():\n",
        "  setup_env_oct2Hist_usemodel = True\n",
        "  !git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-ModelInference\n",
        "  %cd OCT2Hist-ModelInference\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import oct2hist\n",
        "from utils.show_images import *\n",
        "from google.colab import drive\n",
        "from utils.crop import crop\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "KpOc9uoU27Ol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "outputId": "95f067f4-50d6-4f34-a229-4df3828aea29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OCT2Hist-ModelInference'...\n",
            "remote: Enumerating objects: 610, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 610 (delta 58), reused 1 (delta 0), pack-reused 504\u001b[K\n",
            "Receiving objects: 100% (610/610), 20.28 MiB | 20.63 MiB/s, done.\n",
            "Resolving deltas: 100% (315/315), done.\n",
            "Submodule 'pytorch-CycleGAN-and-pix2pix' (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) registered for path 'pytorch-CycleGAN-and-pix2pix'\n",
            "Cloning into '/content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2513, done.        \n",
            "remote: Total 2513 (delta 0), reused 0 (delta 0), pack-reused 2513        \n",
            "Receiving objects: 100% (2513/2513), 8.20 MiB | 14.17 MiB/s, done.\n",
            "Resolving deltas: 100% (1575/1575), done.\n",
            "Submodule path 'pytorch-CycleGAN-and-pix2pix': checked out '9f8f61e5a375c2e01c5187d093ce9c2409f409b0'\n",
            "/content/OCT2Hist-ModelInference\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1edfda801d60>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_images\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'crop' from 'utils.crop' (/content/OCT2Hist-ModelInference/utils/crop.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the network environment\n",
        "oct2hist.setup_network()"
      ],
      "metadata": {
        "id": "ioK7xb4n79wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run\n",
        "In this step we load the OCT image, then crops it to the right size\n",
        "\n"
      ],
      "metadata": {
        "id": "FYlb7hiAYnev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OCT image\n",
        "oct_image = cv2.imread(oct_input_image_path)\n",
        "\n",
        "# Cropping\n",
        "cropped = crop(oct_image, width=1024, height=512, x0=x0, z0=z0)\n",
        "\n",
        "# Run Inference\n",
        "virtual_histology_image, masked_image , o2h_input = oct2hist.run_network(\n",
        "    cropped,\n",
        "    microns_per_pixel_x = microns_per_pixel_x,\n",
        "    microns_per_pixel_z = microns_per_pixel_z )"
      ],
      "metadata": {
        "id": "0iKHLqfhtmmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Masked Image\n",
        "showImg(masked_image)"
      ],
      "metadata": {
        "id": "Fs7PGRr38zWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pre Processing Results (before - left, after preprocessing - right)\n",
        "showTwoImgs(oct_image, o2h_input)"
      ],
      "metadata": {
        "id": "o6_M4WLbmume"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Final Results\n",
        "showTwoImgs(cropped, virtual_histology_image)"
      ],
      "metadata": {
        "id": "SBTbZrN5tBUa"
      }
    }
  ]
}