{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Use this notebook to convert OCT 3D volume (stored in .tiff)  to virtual histology.\n",
        "\n",
        "To get started,\n",
        "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/OCT2Hist-ModelInference/blob/main/run_oct2hist3D.ipynb) and run.\n"
      ],
      "metadata": {
        "id": "NQOdtmQ3laV6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "# @title Notebook Inputs { display-mode: \"form\" }\n",
        "# @markdown Input Image Path\n",
        "# Path to the OCT image\n",
        "oct_input_volume_path = \"/content/drive/Shareddrives/Yolab - Current Projects/Aidan/2023 Alopecia Test/AlopeciaTest-Region1.tif\" # @param {type:\"string\"}\n",
        "\n",
        "# OCT image's pixel size\n",
        "microns_per_pixel_z = 1\n",
        "microns_per_pixel_x = 1\n",
        "\n",
        "# @markdown Cropping Parameters\n",
        "x0 = 0 # @param {type:\"slider\", min:0, max:1000, step:10}\n",
        "z0 = 50 # @param {type:\"slider\", min:0, max:1000, step:10}\n",
        "\n",
        "output_tiff_path = \"/content/drive/Shareddrives/Yolab - Current Projects/Yonatan/Output/\" # @param {type:\"string\"}\n",
        "\n",
        "min_signal_threshold = 0.46"
      ],
      "metadata": {
        "id": "uSag75ONOOgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Environment"
      ],
      "metadata": {
        "id": "UNHZ8Sbinu1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'setup_env_oct2Hist_usemodel' in globals():\n",
        "  setup_env_oct2Hist_usemodel = True\n",
        "  !git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-ModelInference\n",
        "  !pip install imageio\n",
        "  %cd OCT2Hist-ModelInference\n",
        "\n",
        "import cv2\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import oct2hist\n",
        "from utils.show_images import *\n",
        "from google.colab import drive\n",
        "from utils.crop import crop\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "KpOc9uoU27Ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53a82559-4766-4413-d411-72a0e7938fb6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OCT2Hist-ModelInference'...\n",
            "remote: Enumerating objects: 642, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 642 (delta 77), reused 1 (delta 0), pack-reused 504\u001b[K\n",
            "Receiving objects: 100% (642/642), 21.54 MiB | 16.40 MiB/s, done.\n",
            "Resolving deltas: 100% (334/334), done.\n",
            "Submodule 'pytorch-CycleGAN-and-pix2pix' (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) registered for path 'pytorch-CycleGAN-and-pix2pix'\n",
            "Cloning into '/content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2513, done.        \n",
            "remote: Total 2513 (delta 0), reused 0 (delta 0), pack-reused 2513        \n",
            "Receiving objects: 100% (2513/2513), 8.20 MiB | 11.13 MiB/s, done.\n",
            "Resolving deltas: 100% (1575/1575), done.\n",
            "Submodule path 'pytorch-CycleGAN-and-pix2pix': checked out '9f8f61e5a375c2e01c5187d093ce9c2409f409b0'\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (2.31.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio) (1.23.5)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio) (9.4.0)\n",
            "/content/OCT2Hist-ModelInference\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the network environment\n",
        "oct2hist.setup_network()"
      ],
      "metadata": {
        "id": "ioK7xb4n79wZ",
        "outputId": "98ffad0a-22f9-459f-c0fa-89752476305d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run\n",
        "In this step we load the OCT image, then crops it to the right size\n",
        "\n"
      ],
      "metadata": {
        "id": "FYlb7hiAYnev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load OCT image\n",
        "_, oct_volume = cv2.imreadmulti(oct_input_volume_path)\n",
        "oct_volume = np.asarray(oct_volume)\n",
        "\n",
        "# Loop over entire volume convert and append to outputfile\n",
        "for i in range(oct_volume.shape[0]):\n",
        "  filepath = output_tiff_path + \"{0:04}.png\".format(i)\n",
        "  print(filepath)\n",
        "\n",
        "  # Crop\n",
        "  oct_image = cv2.cvtColor(oct_volume[i,:,:], cv2.COLOR_GRAY2BGR)\n",
        "  cropped = crop(oct_image, target_width=1024, target_height=512, x0=x0, z0=z0)\n",
        "\n",
        "  # Run Inference\n",
        "  virtual_histology_image, *_ = oct2hist.run_network(\n",
        "    cropped,\n",
        "    microns_per_pixel_x = microns_per_pixel_x,\n",
        "    microns_per_pixel_z = microns_per_pixel_z,\n",
        "    min_signal_threshold = min_signal_threshold\n",
        "    )\n",
        "\n",
        "  # Save to file\n",
        "  filepath = output_tiff_path + \"{0:04}.png\".format(i)\n",
        "  bgr_image = cv2.cvtColor(virtual_histology_image, cv2.COLOR_RGB2BGR)\n",
        "  cv2.imwrite(filepath, bgr_image)"
      ],
      "metadata": {
        "id": "0iKHLqfhtmmf",
        "outputId": "5041e660-50e0-4976-ea15-f1ef592aed3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 684
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/Yolab - Current Projects/Yonatan/Output/0000.png\n",
            "python /content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix//test.py --netG resnet_9blocks --dataroot \"/content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix//dataset/\"  --model pix2pix --name oct2hist --checkpoints_dir \"/content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix//checkpoints\" --results_dir \"/content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix//results\" --num_test 1000\n",
            "Command failed with exit code 1.\n",
            "Error message:\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix//test.py\", line 43, in <module>\n",
            "    opt = TestOptions().parse()  # get test options\n",
            "  File \"/content/OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix/options/base_options.py\", line 136, in parse\n",
            "    torch.cuda.set_device(opt.gpu_ids[0])\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 350, in set_device\n",
            "    torch._C._cuda_setDevice(device)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\", line 247, in _lazy_init\n",
            "    torch._C._cuda_init()\n",
            "RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bd108dae7269>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# Run Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   virtual_histology_image, *_ = oct2hist.run_network(\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mcropped\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmicrons_per_pixel_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmicrons_per_pixel_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OCT2Hist-ModelInference/oct2hist.py\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m(oct_image, microns_per_pixel_x, microns_per_pixel_z, apply_masking, min_signal_threshold, apply_gray_level_scaling, appy_resolution_matching)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0;31m# Run the neural net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m   \u001b[0mvirtual_histology_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpix2pix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo2h_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"oct2hist\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;31m# Post process, return image to original size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OCT2Hist-ModelInference/utils/pix2pix.py\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m(im, model_name, netG_flag)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Run pix2pix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0m_run_subprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'python {base_folder}/test.py {netG_flag} --dataroot \"{base_folder}/dataset/\"  --model pix2pix --name {model_name} --checkpoints_dir \"{base_folder}/checkpoints\" --results_dir \"{base_folder}/results\" --num_test 1000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;31m# Load output image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OCT2Hist-ModelInference/utils/pix2pix.py\u001b[0m in \u001b[0;36m_run_subprocess\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error message:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"See error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Create folder if it doesn't exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: See error"
          ]
        }
      ]
    }
  ]
}